{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d212c2c5-414d-4627-812a-68bce5454b6d",
      "metadata": {
        "id": "d212c2c5-414d-4627-812a-68bce5454b6d",
        "outputId": "8b119197-af1e-4bdc-89b5-83a310da60f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model parameters have been scaled and saved to ./results/model_parameters_scaled_averaged.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\alexp\\AppData\\Local\\Temp\\ipykernel_20708\\570735738.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Load the model parameters from the .pth file\n",
        "model_path = \"./results/model_averaged.pth\"\n",
        "model_state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "\n",
        "# Initialize global max and min\n",
        "global_max = -np.inf\n",
        "global_min = np.inf\n",
        "\n",
        "# First pass: Find global max and min\n",
        "for param in model_state_dict.values():\n",
        "    param_np = param.cpu().numpy()\n",
        "    param_max = np.max(param_np)\n",
        "    param_min = np.min(param_np)\n",
        "\n",
        "    # Update global max and min\n",
        "    global_max = max(global_max, param_max)\n",
        "    global_min = min(global_min, param_min)\n",
        "\n",
        "# Define rescaling and rounding function\n",
        "def rescale_and_round(value, min_val, max_val, new_min=-128, new_max=127):\n",
        "    scaled_value = ((value - min_val) / (max_val - min_val)) * (new_max - new_min) + new_min\n",
        "    return np.round(scaled_value).astype(int)  # Round to nearest integer and cast to int\n",
        "\n",
        "def decimal_to_signed_binary(value):\n",
        "\n",
        "    # If the value is negative, calculate two's complement\n",
        "    if value < 0:\n",
        "        value = (1 << 8) + value  # Two's complement for negative values\n",
        "\n",
        "    # Format as an 8-bit binary string\n",
        "    return f\"{value:08b}\"\n",
        "\n",
        "# Specify output file for saving scaled weights and biases\n",
        "output_file = \"./results/model_parameters_scaled_averaged.txt\"\n",
        "\n",
        "# Second pass: Write rescaled parameters to file\n",
        "with open(output_file, \"w\") as f:\n",
        "\n",
        "    f.write(f\"Global Max: {global_max:.8f}\\n\")\n",
        "    f.write(f\"Global Min: {global_min:.8f}\\n\")\n",
        "\n",
        "    for name, param in model_state_dict.items():\n",
        "        # Convert the tensor to a NumPy array\n",
        "        param_np = param.cpu().numpy()\n",
        "\n",
        "        # Rescale parameters\n",
        "        scaled_param_np = rescale_and_round(param_np, global_min, global_max)\n",
        "\n",
        "        val_binary = decimal_to_signed_binary(val)\n",
        "\n",
        "        # Write name as 'name = [...]'\n",
        "        f.write(f\"{name} = [\\n{{\\n\")\n",
        "\n",
        "        # Write shape and global max/min values\n",
        "        f.write(f\"  Shape: {param_np.shape}\\n\")\n",
        "\n",
        "        # Write rescaled weights/biases\n",
        "        f.write(\"  Scaled Weights/Biases:\\n\")\n",
        "\n",
        "        # Check if the parameter is a matrix (2D) or vector (1D) and format accordingly\n",
        "        if scaled_param_np.ndim == 2:  # Matrix case\n",
        "            for row in scaled_param_np:\n",
        "                formatted_row = \", \".join(f\"8'sb{val_binary}\" for val in row) + \",\"  # Format each value with comma\n",
        "                f.write(f\"    {formatted_row}\\n\")\n",
        "        elif scaled_param_np.ndim == 1:  # Vector case\n",
        "            formatted_vector = \", \".join(f\"8'sb{val_binary}\" for val in scaled_param_np) + \",\"  # Format each value with comma\n",
        "            f.write(f\"    {formatted_vector}\\n\")\n",
        "        else:\n",
        "            f.write(f\"    {scaled_param_np}\\n\")\n",
        "\n",
        "        # Close the block with braces\n",
        "        f.write(\"}\\n]\\n\\n\")\n",
        "\n",
        "print(f\"Model parameters have been scaled and saved to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a8c50e0-f861-417a-b57d-5ed89823eb5a",
      "metadata": {
        "id": "9a8c50e0-f861-417a-b57d-5ed89823eb5a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}